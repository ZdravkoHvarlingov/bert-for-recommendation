{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "236afb4c60c7ec613449c67b2c4cd5af5ca37658326c13f26b7a5944e23c45d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 11\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 149
    }
   ],
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "source": [
    "# Similar book dataset and dataloader"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_book_pairs = pd.read_csv('data/similar_book_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarBooksDataset(Dataset):\n",
    "\n",
    "  def __init__(self, similar_book_pairs):\n",
    "    self.similar_book_pairs = similar_book_pairs\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.similar_book_pairs)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    pair = self.similar_book_pairs.iloc[item]\n",
    "    \n",
    "    return {\n",
    "        'book1_sequence': f'\"{pair[\"book1_title\"]}\" - {pair[\"book1_description\"]}',\n",
    "        'book2_sequence': f'\"{pair[\"book2_title\"]}\" - {pair[\"book2_description\"]}',\n",
    "        'target_class': 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceCollate:\n",
    "    \"\"\"\n",
    "    Collate to tokenize and apply the padding to the sequences with dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        book1_sequences, book2_sequences, target_classes = [], [], []\n",
    "        for pair in batch:\n",
    "            book1_sequences.append(pair['book1_sequence'])\n",
    "            book2_sequences.append(pair['book2_sequence'])\n",
    "            target_classes.append(pair['target_class'])\n",
    "\n",
    "        encoded_sequences = self.tokenizer(book1_sequences, book2_sequences, padding='longest', truncation='longest_first', return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'sequences': encoded_sequences['input_ids'],\n",
    "            'attention_masks': encoded_sequences['attention_mask'],\n",
    "            'target_classes': torch.as_tensor(target_classes, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SimilarBooksDataset(similar_book_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=4, collate_fn=SequenceCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 488])\ntorch.Size([4, 488])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 336])\ntorch.Size([4, 336])\ntorch.Size([4])\ntorch.Size([4, 445])\ntorch.Size([4, 445])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 250])\ntorch.Size([4, 250])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 476])\ntorch.Size([4, 476])\ntorch.Size([4])\ntorch.Size([4, 504])\ntorch.Size([4, 504])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 472])\ntorch.Size([4, 472])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 449])\ntorch.Size([4, 449])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 417])\ntorch.Size([4, 417])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 490])\ntorch.Size([4, 490])\ntorch.Size([4])\ntorch.Size([4, 422])\ntorch.Size([4, 422])\ntorch.Size([4])\ntorch.Size([4, 497])\ntorch.Size([4, 497])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 296])\ntorch.Size([4, 296])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\ntorch.Size([4, 381])\ntorch.Size([4, 381])\ntorch.Size([4])\ntorch.Size([4, 392])\ntorch.Size([4, 392])\ntorch.Size([4])\ntorch.Size([4, 432])\ntorch.Size([4, 432])\ntorch.Size([4])\ntorch.Size([4, 473])\ntorch.Size([4, 473])\ntorch.Size([4])\ntorch.Size([4, 512])\ntorch.Size([4, 512])\ntorch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for item in dl:\n",
    "    print(item['sequences'].shape)\n",
    "    print(item['attention_masks'].shape)\n",
    "    print(item['target_classes'].shape)\n",
    "    count += 1\n",
    "    if count >= 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}