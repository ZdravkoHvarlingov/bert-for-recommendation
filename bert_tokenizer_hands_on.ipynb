{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "236afb4c60c7ec613449c67b2c4cd5af5ca37658326c13f26b7a5944e23c45d9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_book1_desc = 'An awesome book about the nature. There is no more you can have!'\n",
    "sample_book2_desc = 'Do you want to experience some horror stories, if yes - do not hesitate to try this book out!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Sentence: An awesome book about the nature. There is no more you can have!\n   Tokens: ['An', 'awesome', 'book', 'about', 'the', 'nature', '.', 'There', 'is', 'no', 'more', 'you', 'can', 'have', '!']\nToken IDs: [1760, 14918, 1520, 1164, 1103, 2731, 119, 1247, 1110, 1185, 1167, 1128, 1169, 1138, 106]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sample_book1_desc)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f' Sentence: {sample_book1_desc}')\n",
    "print(f'   Tokens: {tokens}')\n",
    "print(f'Token IDs: {token_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(('[SEP]', 102), ('[CLS]', 101), ('[PAD]', 0), ('[UNK]', 100))"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "(tokenizer.sep_token, tokenizer.sep_token_id), (tokenizer.cls_token, tokenizer.cls_token_id), (tokenizer.pad_token, tokenizer.pad_token_id), (tokenizer.unk_token, tokenizer.unk_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': [101, 1760, 14918, 1520, 1164, 1103, 2731, 119, 1247, 1110, 1185, 1167, 1128, 1169, 1138, 106, 102, 2091, 1128, 1328, 1106, 2541, 1199, 5367, 2801, 117, 1191, 4208, 118, 1202, 1136, 17467, 1106, 2222, 1142, 1520, 1149, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"[CLS] An awesome book about the nature. There is no more you can have! [SEP] Do you want to experience some horror stories, if yes - don't hesitate to try this book out! [SEP]\""
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "encoded_input = tokenizer(sample_book1_desc, sample_book2_desc)\n",
    "print(encoded_input)\n",
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sentences = [\"Hello I'm a single sentence. You are not!\",\n",
    "                   \"And another sentence\",\n",
    "                   \"And the very very last one\"]\n",
    "batch_of_second_sentences = [\"I'm a sentence that goes with the first sentence\",\n",
    "                             \"And I should be encoded with the second sentence\",\n",
    "                             \"And I go with the very last one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[  101,  8667,   146,   112,   182,   170,  1423,  5650,   119,  1192,\n          1132,  1136,   106,   102,   146,   112,   182,   170,  5650,  1115,\n          2947,  1114,  1103,  1148,  5650,   102],\n        [  101,  1262,  1330,  5650,   102,  1262,   146,  1431,  1129, 12544,\n          1114,  1103,  1248,  5650,   102,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  101,  1262,  1103,  1304,  1304,  1314,  1141,   102,  1262,   146,\n          1301,  1114,  1103,  1304,  1314,  1141,   102,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1],\n        [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n         0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n         0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, padding='longest', truncation='longest_first', return_tensors='pt')\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([3, 26])\n26\n[CLS] Hello I'm a single sentence. You are not! [SEP] I'm a sentence that goes with the first sentence [SEP]\n26\n[CLS] And another sentence [SEP] And I should be encoded with the second sentence [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n26\n[CLS] And the very very last one [SEP] And I go with the very last one [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_inputs['input_ids'].shape)\n",
    "for ids in encoded_inputs['input_ids']:\n",
    "    print(len(ids))\n",
    "    print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}